{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1aee5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anguelos/venvs/p311/bin/python\n",
      "/home/anguelos/work/src/pylelemmatize/experiments/delemmatization\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "print(sys.executable)\n",
    "import torch\n",
    "import pylelemmatize\n",
    "import numpy as numpy\n",
    "import string\n",
    "import glob\n",
    "from pathlib import Path\n",
    "!pwd\n",
    "train_models=True\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61a8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"tinyshakespeare_largelines.txt\").is_file():\n",
    "    !wget -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O tinyshakespeare.txt\n",
    "    lines = [l.strip() for l in  open(\"tinyshakespeare.txt\",\"r\").readlines()]\n",
    "    large_lines = []\n",
    "    cur_line=''\n",
    "    for inputline in lines:\n",
    "        inputline = inputline.strip()\n",
    "        if inputline:\n",
    "            cur_line+= (' '+inputline)\n",
    "        if len(cur_line)>50:\n",
    "            large_lines.append(cur_line)\n",
    "            cur_line = ''\n",
    "    open(\"tinyshakespeare_largelines.txt\", \"w\").write(\"\\n\".join(large_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0964ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_model_accuracy(model_path):\n",
    "#    model = pylelemmatize.DemapperLSTM.resume(model_path)\n",
    "#    epoch_accuracies = sorted(model.history['valid_acc'].items())\n",
    "#    return epoch_accuracies[-1][1]\n",
    "\n",
    "\n",
    "#model_path = \"./models/mufi2asciinocase_koeningsffull_simple.pt\"\n",
    "#acc=get_model_accuracy(model_path)\n",
    "#cer=(100*(1 - acc))\n",
    "#print(f\"CER: {cer:.2f} %\")\n",
    "#f\"{100*(1-(cer/4.04)):.2f} %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11db1910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNIING FOR ./models/ascii2tinyconsonants_shakespare_simpledeep.pt\n",
      "Running on cuda\n",
      "Corpus loaded: 14961 lines, 1093182 characters, 64 unique characters.\n",
      "thas as a tast. wath sama � charactars. and thaar � raplacamants.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '.', '$': '.', '&': '.', \"'\": '.', ',': '.', '-': '.', '.': '.', '3': '1', ':': '.', ';': '.', '?': '.', 'A': 'a', 'B': 'b', 'C': 'c', 'D': 'd', 'E': 'a', 'F': 'f', 'G': 'g', 'H': 'h', 'I': 'a', 'J': 'j', 'K': 'k', 'L': 'l', 'M': 'm', 'N': 'n', 'O': 'a', 'P': 'p', 'Q': 'q', 'R': 'r', 'S': 's', 'T': 't', 'U': 'a', 'V': 'v', 'W': 'w', 'X': 'x', 'Y': 'y', 'Z': 'z', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'a', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'a', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'a', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'a', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z'}, unknown_chr='�') Num outputs: 25 Num inputs: 64 assignments: 64\n",
      "Dataset loaded: Items 14961, CER 0.3365% , Input alphabet:  .1abcdfghjklmnpqrstvwxyz, Output alphabet:  !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Training Dataset : Lines 11968, Characters 873370, CER 0.3371%\n",
      "Validation Dataset : Lines 2993, Characters 219812, CER 0.3339%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : ta daa apan tha bad my fathar daad. ta laa clasa by has hanast banas. bat naw\n",
      "Source decoded: ta daa apan tha bad my fathar daad. ta laa clasa by has hanast banas. bat naw\n",
      "Target        : To die upon the bed my father died, To lie close by his honest bones: but now\n",
      "Target decoded: To die upon the bed my father died, To lie close by his honest bones: but now\n",
      "Source Tensor: tensor([20,  4,  1,  7,  4,  4,  1,  4, 16,  4, 15,  1, 20, 10,  4,  1,  5,  4,\n",
      "         7,  1, 14, 24,  1,  8,  4, 20, 10,  4, 18,  1,  7,  4,  4,  7,  2,  1,\n",
      "        20,  4,  1, 13,  4,  4,  1,  6, 13,  4, 19,  4,  1,  5, 24,  1, 10,  4,\n",
      "        19,  1, 10,  4, 15,  4, 19, 20,  1,  5,  4, 15,  4, 19,  2,  1,  5,  4,\n",
      "        20,  1, 15,  4, 22])\n",
      "Target Tensor: tensor([32, 53,  1, 42, 47, 43,  1, 59, 54, 53, 52,  1, 58, 46, 43,  1, 40, 43,\n",
      "        42,  1, 51, 63,  1, 44, 39, 58, 46, 43, 56,  1, 42, 47, 43, 42,  6,  1,\n",
      "        32, 53,  1, 50, 47, 43,  1, 41, 50, 53, 57, 43,  1, 40, 63,  1, 46, 47,\n",
      "        57,  1, 46, 53, 52, 43, 57, 58,  1, 40, 53, 52, 43, 57, 10,  1, 40, 59,\n",
      "        58,  1, 52, 53, 61])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.3365\n",
      "DemapperLSTM(input_alphabet=' .1abcdfghjklmnpqrstvwxyz', output_alphabet=\" !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\", hidden_sizes=[128, 128, 128, 128], dropout=[0.1, 0.1, 0.1, 0.1])Epoch: 100\n",
      "\n",
      "Train Accuracy: 0.9525951200522116 \n",
      "Valid Accuracy: 0.9534875257037833 \n",
      "Train Loss: 0.13395238657536285 \n",
      "Valid Loss: 0.14081502934037826 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_models = True\n",
    "!mkdir -p ./models/\n",
    "\n",
    "corpus_name = \"shakespare\"\n",
    "corpus_files = set(glob.glob(\"tinyshakespeare_largelines.txt\"))\n",
    "max_trainset_sz = -1\n",
    "nb_epochs = 100\n",
    "\n",
    "archname = \"simpledeep\"\n",
    "hidden_sizes=\"128,128,128,128\" ; dropouts=\"0.1,0.1,0.1,0.1\"\n",
    "\n",
    "custom_map=''\n",
    "\n",
    "mapname = \"ascii2tinyconsonants\"\n",
    "input_alphabet=pylelemmatize.charset.ascii\n",
    "#output_alphabet=' ,.'+string.ascii_lowercase+string.digits\n",
    "output_alphabet=\" 1.abcdfghjklmnpqrstvwxyz\"\n",
    "output_model_path = f\"./models/{mapname}_{corpus_name}_{archname}.pt\"\n",
    "print(f\"RUNIING FOR {output_model_path}\")\n",
    "if train_models:\n",
    "    pylelemmatize.main_train_one2one(argv=[\"NA\"],\n",
    "        input_alphabet=input_alphabet, output_alphabet=output_alphabet, custom_map=custom_map,\n",
    "        corpus_files=corpus_files, max_trainset_sz=max_trainset_sz,\n",
    "        nb_epochs=nb_epochs,\n",
    "        hidden_sizes=hidden_sizes, dropouts=dropouts,\n",
    "        pseudo_batch_size=10, lr=0.003,\n",
    "        output_model_path=output_model_path\n",
    "                                     ,min_char_similarity=.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eeac8b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b",
      "\f",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 26166 lines, 2946140 characters, 146 unique characters.\n",
      "this is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '&': '&', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', ';': ';', '=': '=', '>': '>', '?': '?', 'A': 'a', 'B': 'b', 'C': 'c', 'D': 'd', 'E': 'e', 'F': 'f', 'G': 'g', 'H': 'h', 'I': 'i', 'J': 'j', 'K': 'k', 'L': 'l', 'M': 'm', 'N': 'n', 'O': 'o', 'P': 'p', 'Q': 'q', 'R': 'r', 'S': 's', 'T': 't', 'U': 'u', 'V': 'v', 'W': 'w', 'X': 'x', 'Y': 'y', 'Z': 'z', '[': '[', '\\\\': '\\\\', ']': ']', '_': '_', '`': '`', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '|': '|', '¬': '!', '¶': 'p', 'À': 'a', 'Â': 'a', 'Ä': 'a', 'È': 'e', 'Ë': 'e', 'Ò': 'o', 'Ö': 'o', 'Ù': 'u', 'Û': 'u', 'Ü': 'u', 'ß': 's', 'à': 'a', 'á': 'a', 'â': 'a', 'ä': 'a', 'æ': 'a', 'è': 'e', 'é': 'e', 'ê': 'e', 'ë': 'e', 'ì': 'i', 'î': 'i', 'ï': 'i', 'ñ': 'n', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ÿ': 'y', 'ā': 'a', 'ē': 'e', 'ħ': 'h', 'ī': 'i', 'ō': 'o', 'ū': 'u', 'ŵ': 'w', 'ȝ': 'y', 'ɉ': 'j', 'ʼ': \"'\", '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ͨ': 'c', '₰': 'p', 'ꝑ': 'p', 'ꝓ': 'p', 'ꝙ': 'q', 'ꝝ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's', 'ꝯ': 'o', 'ꝰ': '�'}, unknown_chr='�') Num outputs: 58 Num inputs: 143 assignments: 143\n",
      "Dataset loaded: Items 21580, CER 0.0481% , Input alphabet:  !&'(),-./0123456789:;=>?[\\]_`abcdefghijklmnopqrstuvwxyz|, Output alphabet:  !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\n",
      "Training Dataset : Lines 17264, Characters 2247608, CER 0.0481%\n",
      "Validation Dataset : Lines 4316, Characters 561272, CER 0.0482%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : doch wie oben vergriffen ist, � das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren � unnd deren gotzhusz sannct blasien � an iren brieven, rodeln,\n",
      "Source decoded: doch wie oben vergriffen ist, � das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren � unnd deren gotzhusz sannct blasien � an iren brieven, rodeln,\n",
      "Target        : doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Target decoded: doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Source Tensor: tensor([34, 45, 33, 38,  1, 53, 39, 35,  1, 45, 32, 35, 44,  1, 52, 35, 48, 37,\n",
      "        48, 39, 36, 36, 35, 44,  1, 39, 49, 50,  7,  1,  0,  1, 34, 31, 49,  1,\n",
      "        49, 45, 42, 42, 39, 33, 38, 35, 49,  1, 45, 44, 35, 44,  1, 31, 42, 42,\n",
      "        35, 44,  1, 44, 31, 33, 38, 50, 35, 39, 42,  7,  1, 49, 33, 38, 31, 34,\n",
      "        35, 44,  1, 51, 44, 34,  1, 31, 32, 37, 31, 44, 37,  1, 51, 44, 49, 35,\n",
      "        48, 49,  1, 37, 44, 35, 34, 39, 37, 35, 44,  1, 38, 35, 48, 48, 35, 44,\n",
      "         1,  0,  1, 51, 44, 44, 34,  1, 34, 35, 48, 35, 44,  1, 37, 45, 50, 56,\n",
      "        38, 51, 49, 56,  1, 49, 31, 44, 44, 33, 50,  1, 32, 42, 31, 49, 39, 35,\n",
      "        44,  1,  0,  1, 31, 44,  1, 39, 48, 35, 44,  1, 32, 48, 39, 35, 52, 35,\n",
      "        44,  7,  1, 48, 45, 34, 35, 42, 44,  7])\n",
      "Target Tensor: tensor([ 59,  70,  58,  63,   1,  78,  64,  60,   1,  70,  57,  60,  69,   1,\n",
      "         77,  60,  73,  62,  73,  64,  61,  61,  60,  69,   1,  64,  74,  75,\n",
      "          6,   1, 140,   1,  59,  56,  74,   1,  74,  70,  67,  67,  64,  58,\n",
      "         63,  60,  74,   1,  70,  69,  60,  69,   1,  56,  67,  67,  60,  69,\n",
      "          1,  69,  56,  58,  63,  75,  60,  64,  67,   6,   1,  74,  58,  63,\n",
      "         56,  59,  60,  69,   1,  76,  69,  59,   1,  56,  57,  62,  56,  69,\n",
      "         62,   1,  76,  69,  74,  60,  73,  74,   1,  62,  69,  60,  59,  64,\n",
      "         62,  60,  69,   1,  63,  60,  73,  73,  60,  69,   1, 140,   1,  76,\n",
      "         69,  69,  59,   1,  59,  60,  73,  60,  69,   1,  62,  70,  75,  81,\n",
      "         63, 112,  74,  81,   1,  43,  56,  69,  69,  58,  75,   1,  26,  67,\n",
      "         56,  74,  64,  60,  69,   1, 140,   1,  56,  69,   1,  64,  73,  60,\n",
      "         69,   1,  57,  73,  64,  60,  77,  60,  69,   6,   1,  73, 111,  59,\n",
      "         60,  67,  69,   6])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0481\n",
      "DemapperLSTM(input_alphabet=\" !&'(),-./0123456789:;=>?[\\\\]_`abcdefghijklmnopqrstuvwxyz|\", output_alphabet=' !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\\uf2e9\\uf2ea\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 10\n",
      "\n",
      "Train Accuracy: 0.9898999291691434 \n",
      "Valid Accuracy: 0.9902810045753218 \n",
      "Train Loss: 0.03590631989346102 \n",
      "Valid Loss: 0.033795340812641364 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_models = True\n",
    "!mkdir -p ./models/\n",
    "\n",
    "corpus_name = \"koeningsffull\"\n",
    "corpus_files = set(glob.glob(\"../../tmp/koeningsfelden/koenigsfelden_1308-1662_expanded/*txt\"))\n",
    "max_trainset_sz = -1\n",
    "nb_epochs = 10\n",
    "\n",
    "archname = \"simple\"\n",
    "hidden_sizes=\"128,128,128\" ; dropouts=\"0.1,0.1,0.1\"\n",
    "\n",
    "\n",
    "\n",
    "mapname = \"mufi2asciinocase\"\n",
    "input_alphabet=pylelemmatize.charset.mufibmp\n",
    "print(pylelemmatize.charset.ascii)\n",
    "output_alphabet=''.join(set(pylelemmatize.charset.ascii.lower()))\n",
    "custom_map=\"\"\n",
    "\n",
    "if train_models:\n",
    "    pylelemmatize.main_train_one2one(argv=[\"NA\"],\n",
    "        input_alphabet=input_alphabet, output_alphabet=output_alphabet, custom_map=custom_map,\n",
    "        corpus_files=corpus_files, max_trainset_sz=-1,\n",
    "        nb_epochs=nb_epochs,\n",
    "        hidden_sizes=hidden_sizes, dropouts=dropouts,\n",
    "        pseudo_batch_size=10, lr=0.003,\n",
    "        output_model_path=f\"./models/{mapname}_{corpus_name}_{archname}.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ffb10ee-0e31-4e3d-92ab-79d325e9d766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Corpus loaded: 26166 lines, 2946140 characters, 146 unique characters.\n",
      "this is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': ',', '&': ',', '(': '.', ')': '�', ',': ',', '-': '�', '.': '.', '/': ',', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ',', ';': ',', '=': '�', '>': '�', '?': '.', 'A': 'a', 'B': 'b', 'C': 'c', 'D': 'd', 'E': 'e', 'F': 'f', 'G': 'g', 'H': 'h', 'I': 'i', 'J': 'j', 'K': 'k', 'L': 'l', 'M': 'm', 'N': 'n', 'O': 'o', 'P': 'p', 'Q': 'q', 'R': 'r', 'S': 's', 'T': 't', 'U': 'u', 'V': 'v', 'W': 'w', 'X': 'x', 'Y': 'y', 'Z': 'z', '[': '.', '\\\\': '.', ']': '�', '_': '.', '`': '�', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '|': '�', '¬': '�', '¶': 'p', 'À': 'a', 'Â': 'a', 'Ä': 'a', 'È': 'e', 'Ë': 'e', 'Ò': 'o', 'Ö': 'o', 'Ù': 'u', 'Û': 'u', 'Ü': 'u', 'ß': 's', 'à': 'a', 'á': 'a', 'â': 'a', 'ä': 'a', 'æ': 'a', 'è': 'e', 'é': 'e', 'ê': 'e', 'ë': 'e', 'ì': 'i', 'î': 'i', 'ï': 'i', 'ñ': 'n', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ÿ': 'y', 'ā': 'a', 'ē': 'e', 'ħ': 'h', 'ī': 'i', 'ō': 'o', 'ū': 'u', 'ŵ': 'w', 'ȝ': 'y', 'ɉ': 'j', 'ʼ': '�', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ͨ': 'c', '₰': 'p', 'ꝑ': 'p', 'ꝓ': 'p', 'ꝙ': 'q', 'ꝝ': 'a', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 'i', 'ꝯ': 'c', 'ꝰ': '�'}, unknown_chr='�') Num outputs: 40 Num inputs: 143 assignments: 143\n",
      "Dataset loaded: Items 21580, CER 0.0485% , Input alphabet:  ,.0123456789abcdefghijklmnopqrstuvwxyz, Output alphabet:  !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\n",
      "Training Dataset : Lines 17264, Characters 2247608, CER 0.0485%\n",
      "Validation Dataset : Lines 4316, Characters 561272, CER 0.0486%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : doch wie oben vergriffen ist, � das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren � unnd deren gotzhusz sannct blasien � an iren brieven, rodeln,\n",
      "Source decoded: doch wie oben vergriffen ist, � das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren � unnd deren gotzhusz sannct blasien � an iren brieven, rodeln,\n",
      "Target        : doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Target decoded: doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Source Tensor: tensor([17, 28, 16, 21,  1, 36, 22, 18,  1, 28, 15, 18, 27,  1, 35, 18, 31, 20,\n",
      "        31, 22, 19, 19, 18, 27,  1, 22, 32, 33,  2,  1,  0,  1, 17, 14, 32,  1,\n",
      "        32, 28, 25, 25, 22, 16, 21, 18, 32,  1, 28, 27, 18, 27,  1, 14, 25, 25,\n",
      "        18, 27,  1, 27, 14, 16, 21, 33, 18, 22, 25,  2,  1, 32, 16, 21, 14, 17,\n",
      "        18, 27,  1, 34, 27, 17,  1, 14, 15, 20, 14, 27, 20,  1, 34, 27, 32, 18,\n",
      "        31, 32,  1, 20, 27, 18, 17, 22, 20, 18, 27,  1, 21, 18, 31, 31, 18, 27,\n",
      "         1,  0,  1, 34, 27, 27, 17,  1, 17, 18, 31, 18, 27,  1, 20, 28, 33, 39,\n",
      "        21, 34, 32, 39,  1, 32, 14, 27, 27, 16, 33,  1, 15, 25, 14, 32, 22, 18,\n",
      "        27,  1,  0,  1, 14, 27,  1, 22, 31, 18, 27,  1, 15, 31, 22, 18, 35, 18,\n",
      "        27,  2,  1, 31, 28, 17, 18, 25, 27,  2])\n",
      "Target Tensor: tensor([ 59,  70,  58,  63,   1,  78,  64,  60,   1,  70,  57,  60,  69,   1,\n",
      "         77,  60,  73,  62,  73,  64,  61,  61,  60,  69,   1,  64,  74,  75,\n",
      "          6,   1, 140,   1,  59,  56,  74,   1,  74,  70,  67,  67,  64,  58,\n",
      "         63,  60,  74,   1,  70,  69,  60,  69,   1,  56,  67,  67,  60,  69,\n",
      "          1,  69,  56,  58,  63,  75,  60,  64,  67,   6,   1,  74,  58,  63,\n",
      "         56,  59,  60,  69,   1,  76,  69,  59,   1,  56,  57,  62,  56,  69,\n",
      "         62,   1,  76,  69,  74,  60,  73,  74,   1,  62,  69,  60,  59,  64,\n",
      "         62,  60,  69,   1,  63,  60,  73,  73,  60,  69,   1, 140,   1,  76,\n",
      "         69,  69,  59,   1,  59,  60,  73,  60,  69,   1,  62,  70,  75,  81,\n",
      "         63, 112,  74,  81,   1,  43,  56,  69,  69,  58,  75,   1,  26,  67,\n",
      "         56,  74,  64,  60,  69,   1, 140,   1,  56,  69,   1,  64,  73,  60,\n",
      "         69,   1,  57,  73,  64,  60,  77,  60,  69,   6,   1,  73, 111,  59,\n",
      "         60,  67,  69,   6])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0485\n",
      "DemapperLSTM(input_alphabet=' ,.0123456789abcdefghijklmnopqrstuvwxyz', output_alphabet=' !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\\uf2e9\\uf2ea\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 10\n",
      "\n",
      "Train Accuracy: 0.9898274076262409 \n",
      "Valid Accuracy: 0.9899995011331404 \n",
      "Train Loss: 0.03578274693811402 \n",
      "Valid Loss: 0.034229223829069344 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_models = True\n",
    "!mkdir -p ./models/\n",
    "\n",
    "corpus_name = \"koeningsffull\"\n",
    "corpus_files = set(glob.glob(\"../../tmp/koeningsfelden/koenigsfelden_1308-1662_expanded/*txt\"))\n",
    "max_trainset_sz = -1\n",
    "nb_epochs = 10\n",
    "\n",
    "archname = \"simple\"\n",
    "hidden_sizes=\"128,128,128\" ; dropouts=\"0.1,0.1,0.1\"\n",
    "\n",
    "\n",
    "\n",
    "mapname = \"mufi2asciimini\"\n",
    "input_alphabet=pylelemmatize.charset.mufibmp\n",
    "#input_alphabet=pylelemmatize.charset.allbmp_encoding_alphabet_strings[\"ascii\"]\n",
    "output_alphabet=' ,.'+string.ascii_lowercase+string.digits\n",
    "custom_map=\",\".join([f\"{c}:{c.lower()}\" for c in output_alphabet if c!=c.lower()])\n",
    "\n",
    "if train_models:\n",
    "    pylelemmatize.main_train_one2one(argv=[\"NA\"],\n",
    "        input_alphabet=input_alphabet, output_alphabet=output_alphabet, custom_map=custom_map,\n",
    "        corpus_files=corpus_files, max_trainset_sz=max_trainset_sz,\n",
    "        nb_epochs=nb_epochs,\n",
    "        hidden_sizes=hidden_sizes, dropouts=dropouts,\n",
    "        pseudo_batch_size=10, lr=0.003,\n",
    "        output_model_path=f\"./models/{mapname}_{corpus_name}_{archname}.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ed6ea11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Corpus loaded: 26166 lines, 2946140 characters, 146 unique characters.\n",
      "this is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': ',', '&': ',', '(': '.', ')': '�', ',': ',', '-': '�', '.': '.', '/': ',', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ',', ';': ',', '=': '�', '>': '�', '?': '.', 'A': 'a', 'B': 'b', 'C': 'c', 'D': 'd', 'E': 'e', 'F': 'f', 'G': 'g', 'H': 'h', 'I': 'i', 'J': 'j', 'K': 'k', 'L': 'l', 'M': 'm', 'N': 'n', 'O': 'o', 'P': 'p', 'Q': 'q', 'R': 'r', 'S': 's', 'T': 't', 'U': 'u', 'V': 'v', 'W': 'w', 'X': 'x', 'Y': 'y', 'Z': 'z', '[': '.', '\\\\': '.', ']': '�', '_': '.', '`': '�', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '|': '�', '¬': '�', '¶': 'p', 'À': 'a', 'Â': 'a', 'Ä': 'a', 'È': 'e', 'Ë': 'e', 'Ò': 'o', 'Ö': 'o', 'Ù': 'u', 'Û': 'u', 'Ü': 'u', 'ß': 's', 'à': 'a', 'á': 'a', 'â': 'a', 'ä': 'a', 'æ': 'a', 'è': 'e', 'é': 'e', 'ê': 'e', 'ë': 'e', 'ì': 'i', 'î': 'i', 'ï': 'i', 'ñ': 'n', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ÿ': 'y', 'ā': 'a', 'ē': 'e', 'ħ': 'h', 'ī': 'i', 'ō': 'o', 'ū': 'u', 'ŵ': 'w', 'ȝ': 'y', 'ɉ': 'j', 'ʼ': '�', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ͨ': 'c', '₰': 'p', 'ꝑ': 'p', 'ꝓ': 'p', 'ꝙ': 'q', 'ꝝ': 'a', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 'i', 'ꝯ': 'c', 'ꝰ': '�'}, unknown_chr='�') Num outputs: 40 Num inputs: 143 assignments: 143\n",
      "Dataset loaded: Items 21580, CER 0.0485% , Input alphabet:  ,.0123456789abcdefghijklmnopqrstuvwxyz, Output alphabet:  !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\n",
      "Training Dataset : Lines 17264, Characters 2247608, CER 0.0485%\n",
      "Validation Dataset : Lines 4316, Characters 561272, CER 0.0486%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : doch wie oben vergriffen ist, � das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren � unnd deren gotzhusz sannct blasien � an iren brieven, rodeln,\n",
      "Source decoded: doch wie oben vergriffen ist, � das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren � unnd deren gotzhusz sannct blasien � an iren brieven, rodeln,\n",
      "Target        : doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Target decoded: doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Source Tensor: tensor([17, 28, 16, 21,  1, 36, 22, 18,  1, 28, 15, 18, 27,  1, 35, 18, 31, 20,\n",
      "        31, 22, 19, 19, 18, 27,  1, 22, 32, 33,  2,  1,  0,  1, 17, 14, 32,  1,\n",
      "        32, 28, 25, 25, 22, 16, 21, 18, 32,  1, 28, 27, 18, 27,  1, 14, 25, 25,\n",
      "        18, 27,  1, 27, 14, 16, 21, 33, 18, 22, 25,  2,  1, 32, 16, 21, 14, 17,\n",
      "        18, 27,  1, 34, 27, 17,  1, 14, 15, 20, 14, 27, 20,  1, 34, 27, 32, 18,\n",
      "        31, 32,  1, 20, 27, 18, 17, 22, 20, 18, 27,  1, 21, 18, 31, 31, 18, 27,\n",
      "         1,  0,  1, 34, 27, 27, 17,  1, 17, 18, 31, 18, 27,  1, 20, 28, 33, 39,\n",
      "        21, 34, 32, 39,  1, 32, 14, 27, 27, 16, 33,  1, 15, 25, 14, 32, 22, 18,\n",
      "        27,  1,  0,  1, 14, 27,  1, 22, 31, 18, 27,  1, 15, 31, 22, 18, 35, 18,\n",
      "        27,  2,  1, 31, 28, 17, 18, 25, 27,  2])\n",
      "Target Tensor: tensor([ 59,  70,  58,  63,   1,  78,  64,  60,   1,  70,  57,  60,  69,   1,\n",
      "         77,  60,  73,  62,  73,  64,  61,  61,  60,  69,   1,  64,  74,  75,\n",
      "          6,   1, 140,   1,  59,  56,  74,   1,  74,  70,  67,  67,  64,  58,\n",
      "         63,  60,  74,   1,  70,  69,  60,  69,   1,  56,  67,  67,  60,  69,\n",
      "          1,  69,  56,  58,  63,  75,  60,  64,  67,   6,   1,  74,  58,  63,\n",
      "         56,  59,  60,  69,   1,  76,  69,  59,   1,  56,  57,  62,  56,  69,\n",
      "         62,   1,  76,  69,  74,  60,  73,  74,   1,  62,  69,  60,  59,  64,\n",
      "         62,  60,  69,   1,  63,  60,  73,  73,  60,  69,   1, 140,   1,  76,\n",
      "         69,  69,  59,   1,  59,  60,  73,  60,  69,   1,  62,  70,  75,  81,\n",
      "         63, 112,  74,  81,   1,  43,  56,  69,  69,  58,  75,   1,  26,  67,\n",
      "         56,  74,  64,  60,  69,   1, 140,   1,  56,  69,   1,  64,  73,  60,\n",
      "         69,   1,  57,  73,  64,  60,  77,  60,  69,   6,   1,  73, 111,  59,\n",
      "         60,  67,  69,   6])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0485\n",
      "DemapperLSTM(input_alphabet=' ,.0123456789abcdefghijklmnopqrstuvwxyz', output_alphabet=' !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\\uf2e9\\uf2ea\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 10\n",
      "\n",
      "Train Accuracy: 0.9898274076262409 \n",
      "Valid Accuracy: 0.9899995011331404 \n",
      "Train Loss: 0.03578274693811402 \n",
      "Valid Loss: 0.034229223829069344 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_models = True\n",
    "!mkdir -p ./models/\n",
    "\n",
    "corpus_name = \"koeningsffull\"\n",
    "corpus_files = set(glob.glob(\"../../tmp/koeningsfelden/koenigsfelden_1308-1662_expanded/*txt\"))\n",
    "max_trainset_sz = -1\n",
    "nb_epochs = 10\n",
    "\n",
    "archname = \"simple\"\n",
    "hidden_sizes=\"128,128,128\" ; dropouts=\"0.1,0.1,0.1\"\n",
    "\n",
    "\n",
    "\n",
    "mapname = \"mufi2asciimini\"\n",
    "input_alphabet=pylelemmatize.charset.mufibmp\n",
    "#input_alphabet=pylelemmatize.charset.allbmp_encoding_alphabet_strings[\"ascii\"]\n",
    "output_alphabet=' ,.'+string.ascii_lowercase+string.digits\n",
    "custom_map=\",\".join([f\"{c}:{c.lower()}\" for c in output_alphabet if c!=c.lower()])\n",
    "\n",
    "if train_models:\n",
    "    pylelemmatize.main_train_one2one(argv=[\"NA\"],\n",
    "        input_alphabet=input_alphabet, output_alphabet=output_alphabet, custom_map=custom_map,\n",
    "        corpus_files=corpus_files, max_trainset_sz=max_trainset_sz,\n",
    "        nb_epochs=nb_epochs,\n",
    "        hidden_sizes=hidden_sizes, dropouts=dropouts,\n",
    "        pseudo_batch_size=10, lr=0.003,\n",
    "        output_model_path=f\"./models/{mapname}_{corpus_name}_{archname}.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba7584e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Corpus loaded: 26166 lines, 2946140 characters, 146 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '&': '&', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', ';': ';', '=': '=', '>': '>', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'Q': 'Q', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'V', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '[': '[', '\\\\': '\\\\', ']': ']', '_': '_', '`': '`', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'v', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '|': '|', '¬': '¬', '¶': '¶', 'À': 'À', 'Â': 'Â', 'Ä': 'Ä', 'È': 'È', 'Ë': 'Ë', 'Ò': 'Ò', 'Ö': 'Ö', 'Ù': 'Ù', 'Û': 'Û', 'Ü': 'Ü', 'ß': 'ß', 'à': 'à', 'á': 'á', 'â': 'â', 'ä': 'ä', 'æ': 'æ', 'è': 'è', 'é': 'é', 'ê': 'ê', 'ë': 'ë', 'ì': 'ì', 'î': 'î', 'ï': 'ï', 'ñ': 'ñ', 'ò': 'ò', 'ô': 'ô', 'ö': 'ö', 'ù': 'ù', 'û': 'û', 'ü': 'ü', 'ÿ': 'ÿ', 'ā': 'ā', 'ē': 'ē', 'ħ': 'ħ', 'ī': 'ī', 'ō': 'ō', 'ū': 'ū', 'ŵ': 'ŵ', 'ȝ': 'ȝ', 'ɉ': 'ɉ', 'ʼ': 'ʼ', '̃': '̃', '̄': '̄', 'ͣ': 'ͣ', 'ͤ': 'ͤ', 'ͥ': 'ͥ', 'ͦ': 'ͦ', 'ͧ': 'ͧ', 'ͨ': 'ͨ', '₰': '₰', 'ꝑ': 'ꝑ', 'ꝓ': 'ꝓ', 'ꝙ': 'ꝙ', 'ꝝ': 'ꝝ', 'ꝟ': 'ꝟ', 'ꝫ': 'ꝫ', 'ꝭ': 'ꝭ', 'ꝯ': 'ꝯ', 'ꝰ': 'ꝰ'}, unknown_chr='�') Num outputs: 141 Num inputs: 143 assignments: 143\n",
      "Dataset loaded: Items 21580, CER 0.0512% , Input alphabet:  !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTVWXYZ[\\]_`abcdefghijklmnopqrstvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵȝɉʼ̃̄ͣͤͥͦͧͨ₰ꝑꝓꝙꝝꝟꝫꝭꝯꝰ, Output alphabet:  !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\n",
      "Training Dataset : Lines 17264, Characters 2247608, CER 0.0513%\n",
      "Validation Dataset : Lines 4316, Characters 561272, CER 0.0508%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : doch wie oben vergriffen ist, � das solliches onen allen nachteil, schaden vnd abgang vnsers gnedigen herren � vnnd deren gotzhùsz Sannct Blasien � an iren brieven, rödeln,\n",
      "Source decoded: doch wie oben vergriffen ist, � das solliches onen allen nachteil, schaden vnd abgang vnsers gnedigen herren � vnnd deren gotzhùsz Sannct Blasien � an iren brieven, rödeln,\n",
      "Target        : doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Target decoded: doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Source Tensor: tensor([ 58,  69,  57,  62,   1,  76,  63,  59,   1,  69,  56,  59,  68,   1,\n",
      "         75,  59,  72,  61,  72,  63,  60,  60,  59,  68,   1,  63,  73,  74,\n",
      "          6,   1,   0,   1,  58,  55,  73,   1,  73,  69,  66,  66,  63,  57,\n",
      "         62,  59,  73,   1,  69,  68,  59,  68,   1,  55,  66,  66,  59,  68,\n",
      "          1,  68,  55,  57,  62,  74,  59,  63,  66,   6,   1,  73,  57,  62,\n",
      "         55,  58,  59,  68,   1,  75,  68,  58,   1,  55,  56,  61,  55,  68,\n",
      "         61,   1,  75,  68,  73,  59,  72,  73,   1,  61,  68,  59,  58,  63,\n",
      "         61,  59,  68,   1,  62,  59,  72,  72,  59,  68,   1,   0,   1,  75,\n",
      "         68,  68,  58,   1,  58,  59,  72,  59,  68,   1,  61,  69,  74,  79,\n",
      "         62, 110,  73,  79,   1,  43,  55,  68,  68,  57,  74,   1,  26,  66,\n",
      "         55,  73,  63,  59,  68,   1,   0,   1,  55,  68,   1,  63,  72,  59,\n",
      "         68,   1,  56,  72,  63,  59,  75,  59,  68,   6,   1,  72, 109,  58,\n",
      "         59,  66,  68,   6])\n",
      "Target Tensor: tensor([ 59,  70,  58,  63,   1,  78,  64,  60,   1,  70,  57,  60,  69,   1,\n",
      "         77,  60,  73,  62,  73,  64,  61,  61,  60,  69,   1,  64,  74,  75,\n",
      "          6,   1, 140,   1,  59,  56,  74,   1,  74,  70,  67,  67,  64,  58,\n",
      "         63,  60,  74,   1,  70,  69,  60,  69,   1,  56,  67,  67,  60,  69,\n",
      "          1,  69,  56,  58,  63,  75,  60,  64,  67,   6,   1,  74,  58,  63,\n",
      "         56,  59,  60,  69,   1,  76,  69,  59,   1,  56,  57,  62,  56,  69,\n",
      "         62,   1,  76,  69,  74,  60,  73,  74,   1,  62,  69,  60,  59,  64,\n",
      "         62,  60,  69,   1,  63,  60,  73,  73,  60,  69,   1, 140,   1,  76,\n",
      "         69,  69,  59,   1,  59,  60,  73,  60,  69,   1,  62,  70,  75,  81,\n",
      "         63, 112,  74,  81,   1,  43,  56,  69,  69,  58,  75,   1,  26,  67,\n",
      "         56,  74,  64,  60,  69,   1, 140,   1,  56,  69,   1,  64,  73,  60,\n",
      "         69,   1,  57,  73,  64,  60,  77,  60,  69,   6,   1,  73, 111,  59,\n",
      "         60,  67,  69,   6])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0512\n",
      "DemapperLSTM(input_alphabet=' !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTVWXYZ[\\\\]_`abcdefghijklmnopqrstvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵȝɉʼ̃̄ͣͤͥͦͧͨ₰ꝑꝓꝙꝝꝟꝫꝭꝯꝰ', output_alphabet=' !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\\uf2e9\\uf2ea\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 10\n",
      "\n",
      "Train Accuracy: 0.9998820968781033 \n",
      "Valid Accuracy: 0.9998414316053535 \n",
      "Train Loss: 0.0005235365786989514 \n",
      "Valid Loss: 0.0007513156399623788 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_models = True\n",
    "!mkdir -p ./models/\n",
    "\n",
    "corpus_name = \"koeningsffull\"\n",
    "corpus_files = set(glob.glob(\"../../tmp/koeningsfelden/koenigsfelden_1308-1662_expanded/*txt\"))\n",
    "max_trainset_sz = -1\n",
    "nb_epochs = 10\n",
    "\n",
    "archname = \"simple\"\n",
    "hidden_sizes=\"128,128,128\" ; dropouts=\"0.1,0.1,0.1\"\n",
    "\n",
    "\n",
    "\n",
    "mapname = \"mufi2mufiUV\"\n",
    "input_alphabet=pylelemmatize.charset.mufibmp\n",
    "#input_alphabet=pylelemmatize.charset.allbmp_encoding_alphabet_strings[\"ascii\"]\n",
    "#output_alphabet=' ,.'+string.ascii_lowercase+string.digits\n",
    "output_alphabet=pylelemmatize.charset.mufibmp\n",
    "custom_map=\"U:V,u:v\"\n",
    "\n",
    "if train_models:\n",
    "    pylelemmatize.main_train_one2one(argv=[\"NA\"],\n",
    "        input_alphabet=input_alphabet, output_alphabet=output_alphabet, custom_map=custom_map,\n",
    "        corpus_files=corpus_files, max_trainset_sz=max_trainset_sz,\n",
    "        nb_epochs=nb_epochs,\n",
    "        hidden_sizes=hidden_sizes, dropouts=dropouts,\n",
    "        pseudo_batch_size=10, lr=0.003,\n",
    "        output_model_path=f\"./models/{mapname}_{corpus_name}_{archname}.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26bf66f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Corpus loaded: 26166 lines, 2946140 characters, 146 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '&': '&', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', ';': ';', '=': '=', '>': '>', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'Q': 'Q', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'U', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '[': '[', '\\\\': '\\\\', ']': ']', '_': '_', '`': '`', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'v', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '|': '|', '¬': '¬', '¶': '¶', 'À': 'À', 'Â': 'Â', 'Ä': 'Ä', 'È': 'È', 'Ë': 'Ë', 'Ò': 'Ò', 'Ö': 'Ö', 'Ù': 'Ù', 'Û': 'Û', 'Ü': 'Ü', 'ß': 'ß', 'à': 'à', 'á': 'á', 'â': 'â', 'ä': 'ä', 'æ': 'æ', 'è': 'è', 'é': 'é', 'ê': 'ê', 'ë': 'ë', 'ì': 'ì', 'î': 'î', 'ï': 'ï', 'ñ': 'ñ', 'ò': 'ò', 'ô': 'ô', 'ö': 'ö', 'ù': 'ù', 'û': 'û', 'ü': 'ü', 'ÿ': 'ÿ', 'ā': 'ā', 'ē': 'ē', 'ħ': 'ħ', 'ī': 'ī', 'ō': 'ō', 'ū': 'ū', 'ŵ': 'ŵ', 'ȝ': 'ȝ', 'ɉ': 'ɉ', 'ʼ': 'ʼ', '̃': '̃', '̄': '̄', 'ͣ': 'ͣ', 'ͤ': 'ͤ', 'ͥ': 'ͥ', 'ͦ': 'ͦ', 'ͧ': 'ͧ', 'ͨ': 'ͨ', '₰': '₰', 'ꝑ': 'ꝑ', 'ꝓ': 'ꝓ', 'ꝙ': 'ꝙ', 'ꝝ': 'ꝝ', 'ꝟ': 'ꝟ', 'ꝫ': 'ꝫ', 'ꝭ': 'ꝭ', 'ꝯ': 'ꝯ', 'ꝰ': 'ꝰ'}, unknown_chr='�') Num outputs: 141 Num inputs: 143 assignments: 143\n",
      "Dataset loaded: Items 21580, CER 0.0510% , Input alphabet:  !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUWXYZ[\\]_`abcdefghijklmnopqrstvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵȝɉʼ̃̄ͣͤͥͦͧͨ₰ꝑꝓꝙꝝꝟꝫꝭꝯꝰ, Output alphabet:  !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\n",
      "Training Dataset : Lines 17264, Characters 2247608, CER 0.0511%\n",
      "Validation Dataset : Lines 4316, Characters 561272, CER 0.0506%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : doch wie oben vergriffen ist, � das solliches onen allen nachteil, schaden vnd abgang vnsers gnedigen herren � vnnd deren gotzhùsz Sannct Blasien � an iren brieven, rödeln,\n",
      "Source decoded: doch wie oben vergriffen ist, � das solliches onen allen nachteil, schaden vnd abgang vnsers gnedigen herren � vnnd deren gotzhùsz Sannct Blasien � an iren brieven, rödeln,\n",
      "Target        : doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Target decoded: doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Source Tensor: tensor([ 58,  69,  57,  62,   1,  76,  63,  59,   1,  69,  56,  59,  68,   1,\n",
      "         75,  59,  72,  61,  72,  63,  60,  60,  59,  68,   1,  63,  73,  74,\n",
      "          6,   1,   0,   1,  58,  55,  73,   1,  73,  69,  66,  66,  63,  57,\n",
      "         62,  59,  73,   1,  69,  68,  59,  68,   1,  55,  66,  66,  59,  68,\n",
      "          1,  68,  55,  57,  62,  74,  59,  63,  66,   6,   1,  73,  57,  62,\n",
      "         55,  58,  59,  68,   1,  75,  68,  58,   1,  55,  56,  61,  55,  68,\n",
      "         61,   1,  75,  68,  73,  59,  72,  73,   1,  61,  68,  59,  58,  63,\n",
      "         61,  59,  68,   1,  62,  59,  72,  72,  59,  68,   1,   0,   1,  75,\n",
      "         68,  68,  58,   1,  58,  59,  72,  59,  68,   1,  61,  69,  74,  79,\n",
      "         62, 110,  73,  79,   1,  43,  55,  68,  68,  57,  74,   1,  26,  66,\n",
      "         55,  73,  63,  59,  68,   1,   0,   1,  55,  68,   1,  63,  72,  59,\n",
      "         68,   1,  56,  72,  63,  59,  75,  59,  68,   6,   1,  72, 109,  58,\n",
      "         59,  66,  68,   6])\n",
      "Target Tensor: tensor([ 59,  70,  58,  63,   1,  78,  64,  60,   1,  70,  57,  60,  69,   1,\n",
      "         77,  60,  73,  62,  73,  64,  61,  61,  60,  69,   1,  64,  74,  75,\n",
      "          6,   1, 140,   1,  59,  56,  74,   1,  74,  70,  67,  67,  64,  58,\n",
      "         63,  60,  74,   1,  70,  69,  60,  69,   1,  56,  67,  67,  60,  69,\n",
      "          1,  69,  56,  58,  63,  75,  60,  64,  67,   6,   1,  74,  58,  63,\n",
      "         56,  59,  60,  69,   1,  76,  69,  59,   1,  56,  57,  62,  56,  69,\n",
      "         62,   1,  76,  69,  74,  60,  73,  74,   1,  62,  69,  60,  59,  64,\n",
      "         62,  60,  69,   1,  63,  60,  73,  73,  60,  69,   1, 140,   1,  76,\n",
      "         69,  69,  59,   1,  59,  60,  73,  60,  69,   1,  62,  70,  75,  81,\n",
      "         63, 112,  74,  81,   1,  43,  56,  69,  69,  58,  75,   1,  26,  67,\n",
      "         56,  74,  64,  60,  69,   1, 140,   1,  56,  69,   1,  64,  73,  60,\n",
      "         69,   1,  57,  73,  64,  60,  77,  60,  69,   6,   1,  73, 111,  59,\n",
      "         60,  67,  69,   6])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0510\n",
      "DemapperLSTM(input_alphabet=' !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUWXYZ[\\\\]_`abcdefghijklmnopqrstvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵȝɉʼ̃̄ͣͤͥͦͧͨ₰ꝑꝓꝙꝝꝟꝫꝭꝯꝰ', output_alphabet=' !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\\uf2e9\\uf2ea\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 10\n",
      "\n",
      "Train Accuracy: 0.99988476638275 \n",
      "Valid Accuracy: 0.9999126982995766 \n",
      "Train Loss: 0.0004610708983931572 \n",
      "Valid Loss: 0.0005641873861525594 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_models = True\n",
    "!mkdir -p ./models/\n",
    "\n",
    "corpus_name = \"koeningsffull\"\n",
    "corpus_files = set(glob.glob(\"../../tmp/koeningsfelden/koenigsfelden_1308-1662_expanded/*txt\"))\n",
    "max_trainset_sz = -1\n",
    "nb_epochs = 10\n",
    "\n",
    "archname = \"simple\"\n",
    "hidden_sizes=\"128,128,128\" ; dropouts=\"0.1,0.1,0.1\"\n",
    "\n",
    "\n",
    "\n",
    "mapname = \"mufi2mufiVU\"\n",
    "input_alphabet=pylelemmatize.charset.mufibmp\n",
    "#input_alphabet=pylelemmatize.charset.allbmp_encoding_alphabet_strings[\"ascii\"]\n",
    "#output_alphabet=' ,.'+string.ascii_lowercase+string.digits\n",
    "output_alphabet=pylelemmatize.charset.mufibmp\n",
    "custom_map=\"V:U,u:v\"\n",
    "\n",
    "if train_models:\n",
    "    pylelemmatize.main_train_one2one(argv=[\"NA\"],\n",
    "        input_alphabet=input_alphabet, output_alphabet=output_alphabet, custom_map=custom_map,\n",
    "        corpus_files=corpus_files, max_trainset_sz=max_trainset_sz,\n",
    "        nb_epochs=nb_epochs,\n",
    "        hidden_sizes=hidden_sizes, dropouts=dropouts,\n",
    "        pseudo_batch_size=10, lr=0.003,\n",
    "        output_model_path=f\"./models/{mapname}_{corpus_name}_{archname}.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "614650f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Corpus loaded: 26166 lines, 2946140 characters, 146 unique characters.\n",
      "Thjs js a test. wjth some � characters. and thejr � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '&': '&', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', ';': ';', '=': '=', '>': '>', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'J', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'Q': 'Q', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '[': '[', '\\\\': '\\\\', ']': ']', '_': '_', '`': '`', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'j', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '|': '|', '¬': '¬', '¶': '¶', 'À': 'À', 'Â': 'Â', 'Ä': 'Ä', 'È': 'È', 'Ë': 'Ë', 'Ò': 'Ò', 'Ö': 'Ö', 'Ù': 'Ù', 'Û': 'Û', 'Ü': 'Ü', 'ß': 'ß', 'à': 'à', 'á': 'á', 'â': 'â', 'ä': 'ä', 'æ': 'æ', 'è': 'è', 'é': 'é', 'ê': 'ê', 'ë': 'ë', 'ì': 'ì', 'î': 'î', 'ï': 'ï', 'ñ': 'ñ', 'ò': 'ò', 'ô': 'ô', 'ö': 'ö', 'ù': 'ù', 'û': 'û', 'ü': 'ü', 'ÿ': 'ÿ', 'ā': 'ā', 'ē': 'ē', 'ħ': 'ħ', 'ī': 'ī', 'ō': 'ō', 'ū': 'ū', 'ŵ': 'ŵ', 'ȝ': 'ȝ', 'ɉ': 'ɉ', 'ʼ': 'ʼ', '̃': '̃', '̄': '̄', 'ͣ': 'ͣ', 'ͤ': 'ͤ', 'ͥ': 'ͥ', 'ͦ': 'ͦ', 'ͧ': 'ͧ', 'ͨ': 'ͨ', '₰': '₰', 'ꝑ': 'ꝑ', 'ꝓ': 'ꝓ', 'ꝙ': 'ꝙ', 'ꝝ': 'ꝝ', 'ꝟ': 'ꝟ', 'ꝫ': 'ꝫ', 'ꝭ': 'ꝭ', 'ꝯ': 'ꝯ', 'ꝰ': 'ꝰ'}, unknown_chr='�') Num outputs: 141 Num inputs: 143 assignments: 143\n",
      "Dataset loaded: Items 21580, CER 0.0727% , Input alphabet:  !&(),-./0123456789:;=>?ABCDEFGHJKLMNOPQRSTUVWXYZ[\\]_`abcdefghjklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵȝɉʼ̃̄ͣͤͥͦͧͨ₰ꝑꝓꝙꝝꝟꝫꝭꝯꝰ, Output alphabet:  !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\n",
      "Training Dataset : Lines 17264, Characters 2247608, CER 0.0728%\n",
      "Validation Dataset : Lines 4316, Characters 561272, CER 0.0723%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : doch wje oben vergrjffen jst, � das solljches onen allen nachtejl, schaden und abgang unsers gnedjgen herren � unnd deren gotzhùsz Sannct Blasjen � an jren brjeven, rödeln,\n",
      "Source decoded: doch wje oben vergrjffen jst, � das solljches onen allen nachtejl, schaden und abgang unsers gnedjgen herren � unnd deren gotzhùsz Sannct Blasjen � an jren brjeven, rödeln,\n",
      "Target        : doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Target decoded: doch wie oben vergriffen ist, ✳ das solliches onen allen nachteil, schaden und abgang unsers gnedigen herren ✳ unnd deren gotzhùsz Sannct Blasien ✳ an iren brieven, rödeln,\n",
      "Source Tensor: tensor([ 58,  68,  57,  62,   1,  76,  63,  59,   1,  68,  56,  59,  67,   1,\n",
      "         75,  59,  71,  61,  71,  63,  60,  60,  59,  67,   1,  63,  72,  73,\n",
      "          6,   1,   0,   1,  58,  55,  72,   1,  72,  68,  65,  65,  63,  57,\n",
      "         62,  59,  72,   1,  68,  67,  59,  67,   1,  55,  65,  65,  59,  67,\n",
      "          1,  67,  55,  57,  62,  73,  59,  63,  65,   6,   1,  72,  57,  62,\n",
      "         55,  58,  59,  67,   1,  74,  67,  58,   1,  55,  56,  61,  55,  67,\n",
      "         61,   1,  74,  67,  72,  59,  71,  72,   1,  61,  67,  59,  58,  63,\n",
      "         61,  59,  67,   1,  62,  59,  71,  71,  59,  67,   1,   0,   1,  74,\n",
      "         67,  67,  58,   1,  58,  59,  71,  59,  67,   1,  61,  68,  73,  79,\n",
      "         62, 110,  72,  79,   1,  42,  55,  67,  67,  57,  73,   1,  26,  65,\n",
      "         55,  72,  63,  59,  67,   1,   0,   1,  55,  67,   1,  63,  71,  59,\n",
      "         67,   1,  56,  71,  63,  59,  75,  59,  67,   6,   1,  71, 109,  58,\n",
      "         59,  65,  67,   6])\n",
      "Target Tensor: tensor([ 59,  70,  58,  63,   1,  78,  64,  60,   1,  70,  57,  60,  69,   1,\n",
      "         77,  60,  73,  62,  73,  64,  61,  61,  60,  69,   1,  64,  74,  75,\n",
      "          6,   1, 140,   1,  59,  56,  74,   1,  74,  70,  67,  67,  64,  58,\n",
      "         63,  60,  74,   1,  70,  69,  60,  69,   1,  56,  67,  67,  60,  69,\n",
      "          1,  69,  56,  58,  63,  75,  60,  64,  67,   6,   1,  74,  58,  63,\n",
      "         56,  59,  60,  69,   1,  76,  69,  59,   1,  56,  57,  62,  56,  69,\n",
      "         62,   1,  76,  69,  74,  60,  73,  74,   1,  62,  69,  60,  59,  64,\n",
      "         62,  60,  69,   1,  63,  60,  73,  73,  60,  69,   1, 140,   1,  76,\n",
      "         69,  69,  59,   1,  59,  60,  73,  60,  69,   1,  62,  70,  75,  81,\n",
      "         63, 112,  74,  81,   1,  43,  56,  69,  69,  58,  75,   1,  26,  67,\n",
      "         56,  74,  64,  60,  69,   1, 140,   1,  56,  69,   1,  64,  73,  60,\n",
      "         69,   1,  57,  73,  64,  60,  77,  60,  69,   6,   1,  73, 111,  59,\n",
      "         60,  67,  69,   6])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0727\n",
      "DemapperLSTM(input_alphabet=' !&(),-./0123456789:;=>?ABCDEFGHJKLMNOPQRSTUVWXYZ[\\\\]_`abcdefghjklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵȝɉʼ̃̄ͣͤͥͦͧͨ₰ꝑꝓꝙꝝꝟꝫꝭꝯꝰ', output_alphabet=' !&(),-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]_`abcdefghijklmnopqrstuvwxyz|¬¶ÀÂÄÈËÒÖÙÛÜßàáâäæèéêëìîïñòôöùûüÿāēħīōūŵƺȝɉʼˀ̀̂̃̄ͣͤͥͦͧͨ₎₰✳ꝑꝓꝙꝝꝟꝫꝭꝯꝰ\\uf2e9\\uf2ea\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 10\n",
      "\n",
      "Train Accuracy: 0.9998794273734566 \n",
      "Valid Accuracy: 0.9999020082954432 \n",
      "Train Loss: 0.00049064830659729 \n",
      "Valid Loss: 0.0005818227317753773 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_models = True\n",
    "!mkdir -p ./models/\n",
    "\n",
    "corpus_name = \"koeningsffull\"\n",
    "corpus_files = set(glob.glob(\"../../tmp/koeningsfelden/koenigsfelden_1308-1662_expanded/*txt\"))\n",
    "max_trainset_sz = -1\n",
    "nb_epochs = 10\n",
    "\n",
    "archname = \"simple\"\n",
    "hidden_sizes=\"128,128,128\" ; dropouts=\"0.1,0.1,0.1\"\n",
    "\n",
    "\n",
    "\n",
    "mapname = \"mufi2mufiIJ\"\n",
    "input_alphabet=pylelemmatize.charset.mufibmp\n",
    "#input_alphabet=pylelemmatize.charset.allbmp_encoding_alphabet_strings[\"ascii\"]\n",
    "#output_alphabet=' ,.'+string.ascii_lowercase+string.digits\n",
    "output_alphabet=pylelemmatize.charset.mufibmp\n",
    "custom_map=\"I:J,i:j\"\n",
    "\n",
    "if train_models:\n",
    "    pylelemmatize.main_train_one2one(argv=[\"NA\"],\n",
    "        input_alphabet=input_alphabet, output_alphabet=output_alphabet, custom_map=custom_map,\n",
    "        corpus_files=corpus_files, max_trainset_sz=max_trainset_sz,\n",
    "        nb_epochs=nb_epochs,\n",
    "        hidden_sizes=hidden_sizes, dropouts=dropouts,\n",
    "        pseudo_batch_size=10, lr=0.003,\n",
    "        output_model_path=f\"./models/{mapname}_{corpus_name}_{archname}.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381d6dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 20 items.\n",
      "Training Dataset : Lines 20, Characters 2940, CER 0.0276%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 6000\n",
      "\n",
      "Train Accuracy: 1.0 \n",
      "Valid Accuracy: 0.9836204630713723 \n",
      "Train Loss: 2.5606661495203296e-06 \n",
      "Valid Loss: 0.2892725805034126 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0020_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 40 items.\n",
      "Training Dataset : Lines 40, Characters 5793, CER 0.0261%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 3000\n",
      "\n",
      "Train Accuracy: 1.0 \n",
      "Valid Accuracy: 0.9883754271104633 \n",
      "Train Loss: 8.487673826707009e-06 \n",
      "Valid Loss: 0.17827629939321044 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0040_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 60 items.\n",
      "Training Dataset : Lines 60, Characters 8554, CER 0.0281%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 2000\n",
      "\n",
      "Train Accuracy: 1.0 \n",
      "Valid Accuracy: 0.9888294186518841 \n",
      "Train Loss: 2.565450988427642e-05 \n",
      "Valid Loss: 0.15228892117572043 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0060_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 80 items.\n",
      "Training Dataset : Lines 80, Characters 11452, CER 0.0281%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 1500\n",
      "\n",
      "Train Accuracy: 1.0 \n",
      "Valid Accuracy: 0.9898329789013405 \n",
      "Train Loss: 1.560325314571287e-05 \n",
      "Valid Loss: 0.13106618681902713 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0080_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 100 items.\n",
      "Training Dataset : Lines 100, Characters 14153, CER 0.0273%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 1200\n",
      "\n",
      "Train Accuracy: 1.0 \n",
      "Valid Accuracy: 0.989964397505436 \n",
      "Train Loss: 2.8188639978594663e-05 \n",
      "Valid Loss: 0.1258618865427792 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0100_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 125 items.\n",
      "Training Dataset : Lines 125, Characters 17540, CER 0.0278%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 960\n",
      "\n",
      "Train Accuracy: 0.9990877993158495 \n",
      "Valid Accuracy: 0.9894745645265346 \n",
      "Train Loss: 0.006189593528877595 \n",
      "Valid Loss: 0.12591404737238235 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0125_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 150 items.\n",
      "Training Dataset : Lines 150, Characters 20829, CER 0.0279%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 800\n",
      "\n",
      "Train Accuracy: 1.0 \n",
      "Valid Accuracy: 0.9906692791092208 \n",
      "Train Loss: 2.5633201247501347e-05 \n",
      "Valid Loss: 0.10778703962739092 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0150_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 175 items.\n",
      "Training Dataset : Lines 175, Characters 24953, CER 0.0279%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 686\n",
      "\n",
      "Train Accuracy: 0.9995190959002925 \n",
      "Valid Accuracy: 0.9908006977133162 \n",
      "Train Loss: 0.005973077683827017 \n",
      "Valid Loss: 0.10080705707127106 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0175_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 200 items.\n",
      "Training Dataset : Lines 200, Characters 28147, CER 0.0286%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 600\n",
      "\n",
      "Train Accuracy: 0.9999644722350517 \n",
      "Valid Accuracy: 0.9910754820673341 \n",
      "Train Loss: 0.0003611349273705855 \n",
      "Valid Loss: 0.09893599874618468 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0200_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 250 items.\n",
      "Training Dataset : Lines 250, Characters 35480, CER 0.0293%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 480\n",
      "\n",
      "Train Accuracy: 0.9998872604284104 \n",
      "Valid Accuracy: 0.9916728393586772 \n",
      "Train Loss: 0.00024414590686160407 \n",
      "Valid Loss: 0.08833436259685541 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0250_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 300 items.\n",
      "Training Dataset : Lines 300, Characters 42754, CER 0.0298%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 400\n",
      "\n",
      "Train Accuracy: 0.9995322075127473 \n",
      "Valid Accuracy: 0.9916728393586772 \n",
      "Train Loss: 0.0013468362041449684 \n",
      "Valid Loss: 0.07926744022105148 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0300_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 400 items.\n",
      "Training Dataset : Lines 400, Characters 56201, CER 0.0293%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 300\n",
      "\n",
      "Train Accuracy: 0.9998220672230032 \n",
      "Valid Accuracy: 0.9917206279419847 \n",
      "Train Loss: 0.0006070516383280733 \n",
      "Valid Loss: 0.07887568543516398 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0400_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 500 items.\n",
      "Training Dataset : Lines 500, Characters 71264, CER 0.0293%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 240\n",
      "\n",
      "Train Accuracy: 0.9997053210597217 \n",
      "Valid Accuracy: 0.9917086807961578 \n",
      "Train Loss: 0.0007788082368388132 \n",
      "Valid Loss: 0.07494269426317475 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0500_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 600 items.\n",
      "Training Dataset : Lines 600, Characters 85398, CER 0.0291%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 200\n",
      "\n",
      "Train Accuracy: 0.9997775123539193 \n",
      "Valid Accuracy: 0.9916369979211966 \n",
      "Train Loss: 0.0006846961326419887 \n",
      "Valid Loss: 0.07305721003319857 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0600_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 700 items.\n",
      "Training Dataset : Lines 700, Characters 100070, CER 0.0290%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 172\n",
      "\n",
      "Train Accuracy: 0.9996002798041371 \n",
      "Valid Accuracy: 0.9922701966500204 \n",
      "Train Loss: 0.0013784949350403102 \n",
      "Valid Loss: 0.06730310342998731 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0700_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 800 items.\n",
      "Training Dataset : Lines 800, Characters 114153, CER 0.0289%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 150\n",
      "\n",
      "Train Accuracy: 0.9985282909778981 \n",
      "Valid Accuracy: 0.9924135623999426 \n",
      "Train Loss: 0.0037890817306114855 \n",
      "Valid Loss: 0.05045051079009757 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0800_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 900 items.\n",
      "Training Dataset : Lines 900, Characters 128247, CER 0.0289%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 134\n",
      "\n",
      "Train Accuracy: 0.998908356530757 \n",
      "Valid Accuracy: 0.9923179852333277 \n",
      "Train Loss: 0.0028536305557276542 \n",
      "Valid Loss: 0.056110242363248866 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening0900_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 1000 items.\n",
      "Training Dataset : Lines 1000, Characters 142599, CER 0.0290%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 120\n",
      "\n",
      "Train Accuracy: 0.9985483769170892 \n",
      "Valid Accuracy: 0.9923657738166352 \n",
      "Train Loss: 0.003540830609807699 \n",
      "Valid Loss: 0.05053793014491604 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening1000_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 1500 items.\n",
      "Training Dataset : Lines 1500, Characters 214743, CER 0.0291%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 80\n",
      "\n",
      "Train Accuracy: 0.9970057231201949 \n",
      "Valid Accuracy: 0.992903395378844 \n",
      "Train Loss: 0.0077146415825827715 \n",
      "Valid Loss: 0.037444188885307046 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening1500_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 2000 items.\n",
      "Training Dataset : Lines 2000, Characters 285892, CER 0.0290%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 60\n",
      "\n",
      "Train Accuracy: 0.9959320302771676 \n",
      "Valid Accuracy: 0.9935604883993214 \n",
      "Train Loss: 0.011042265334016293 \n",
      "Valid Loss: 0.027617602628406637 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening2000_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 2361 items.\n",
      "Training Dataset : Lines 2361, Characters 338432, CER 0.0293%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 48\n",
      "\n",
      "Train Accuracy: 0.9953993712178517 \n",
      "Valid Accuracy: 0.9938830613366467 \n",
      "Train Loss: 0.013632389333690651 \n",
      "Valid Loss: 0.024442659289841275 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening2500_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 2361 items.\n",
      "Training Dataset : Lines 2361, Characters 338432, CER 0.0293%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 40\n",
      "\n",
      "Train Accuracy: 0.9953107271180031 \n",
      "Valid Accuracy: 0.9940025327949153 \n",
      "Train Loss: 0.014146341428536071 \n",
      "Valid Loss: 0.024035536354146264 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening3000_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 2361 items.\n",
      "Training Dataset : Lines 2361, Characters 338432, CER 0.0293%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 30\n",
      "\n",
      "Train Accuracy: 0.9945631618759455 \n",
      "Valid Accuracy: 0.9940025327949153 \n",
      "Train Loss: 0.017764483216464916 \n",
      "Valid Loss: 0.02236409637071535 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening4000_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 2361 items.\n",
      "Training Dataset : Lines 2361, Characters 338432, CER 0.0293%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 24\n",
      "\n",
      "Train Accuracy: 0.9943829188729199 \n",
      "Valid Accuracy: 0.9936082769826289 \n",
      "Train Loss: 0.01950453555820793 \n",
      "Valid Loss: 0.02252568135197277 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening5000_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': ' ', '!': '!', '(': '(', ')': ')', ',': ',', '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', '?': '?', 'A': 'A', 'B': 'B', 'C': 'C', 'D': 'D', 'E': 'E', 'F': 'F', 'G': 'G', 'H': 'H', 'I': 'I', 'J': 'J', 'K': 'K', 'L': 'L', 'M': 'M', 'N': 'N', 'O': 'O', 'P': 'P', 'R': 'R', 'S': 'S', 'T': 'T', 'U': 'U', 'V': 'V', 'W': 'W', 'X': 'X', 'Y': 'Y', 'Z': 'Z', '\\\\': '\\\\', 'a': 'a', 'b': 'b', 'c': 'c', 'd': 'd', 'e': 'e', 'f': 'f', 'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'k': 'k', 'l': 'l', 'm': 'm', 'n': 'n', 'o': 'o', 'p': 'p', 'q': 'q', 'r': 'r', 's': 's', 't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'x': 'x', 'y': 'y', 'z': 'z', '¬': '!', 'À': 'A', 'Ä': 'A', 'Ò': 'O', 'Ù': 'U', 'Û': 'U', 'à': 'a', 'â': 'a', 'ä': 'a', 'è': 'e', 'ê': 'e', 'ë': 'e', 'î': 'i', 'ò': 'o', 'ô': 'o', 'ö': 'o', 'ù': 'u', 'û': 'u', 'ü': 'u', 'ē': 'e', 'ō': 'o', 'ū': 'u', 'ɉ': 'j', '̃': '�', '̄': '�', 'ͣ': 'a', 'ͤ': 'e', 'ͥ': 'i', 'ͦ': 'o', 'ͧ': 'u', 'ꝟ': 'v', 'ꝫ': 'e', 'ꝭ': 's'}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 2361 items.\n",
      "Training Dataset : Lines 2361, Characters 338432, CER 0.0293%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 16\n",
      "\n",
      "Train Accuracy: 0.9939426531770046 \n",
      "Valid Accuracy: 0.9935724355451483 \n",
      "Train Loss: 0.02204703051368191 \n",
      "Valid Loss: 0.022508319497444113 \n",
      "\n",
      "COMPLETED: ./models/curve_mufi2ascii_koening7500_simple.pt\n",
      "\n",
      "\t\t\t#########################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\n\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': np.str_(' '), '!': np.str_('!'), '(': np.str_('('), ')': np.str_(')'), ',': np.str_(','), '-': np.str_('-'), '.': np.str_('.'), '/': np.str_('/'), '0': np.str_('0'), '1': np.str_('1'), '2': np.str_('2'), '3': np.str_('3'), '4': np.str_('4'), '5': np.str_('5'), '6': np.str_('6'), '7': np.str_('7'), '8': np.str_('8'), '9': np.str_('9'), ':': np.str_(':'), '?': np.str_('?'), 'A': np.str_('A'), 'B': np.str_('B'), 'C': np.str_('C'), 'D': np.str_('D'), 'E': np.str_('E'), 'F': np.str_('F'), 'G': np.str_('G'), 'H': np.str_('H'), 'I': np.str_('I'), 'J': np.str_('J'), 'K': np.str_('K'), 'L': np.str_('L'), 'M': np.str_('M'), 'N': np.str_('N'), 'O': np.str_('O'), 'P': np.str_('P'), 'R': np.str_('R'), 'S': np.str_('S'), 'T': np.str_('T'), 'U': np.str_('U'), 'V': np.str_('V'), 'W': np.str_('W'), 'X': np.str_('X'), 'Y': np.str_('Y'), 'Z': np.str_('Z'), '\\\\': np.str_('\\\\'), 'a': np.str_('a'), 'b': np.str_('b'), 'c': np.str_('c'), 'd': np.str_('d'), 'e': np.str_('e'), 'f': np.str_('f'), 'g': np.str_('g'), 'h': np.str_('h'), 'i': np.str_('i'), 'j': np.str_('j'), 'k': np.str_('k'), 'l': np.str_('l'), 'm': np.str_('m'), 'n': np.str_('n'), 'o': np.str_('o'), 'p': np.str_('p'), 'q': np.str_('q'), 'r': np.str_('r'), 's': np.str_('s'), 't': np.str_('t'), 'u': np.str_('u'), 'v': np.str_('v'), 'w': np.str_('w'), 'x': np.str_('x'), 'y': np.str_('y'), 'z': np.str_('z'), '¬': np.str_('!'), 'À': np.str_('A'), 'Ä': np.str_('A'), 'Ò': np.str_('O'), 'Ù': np.str_('U'), 'Û': np.str_('U'), 'à': np.str_('a'), 'â': np.str_('a'), 'ä': np.str_('a'), 'è': np.str_('e'), 'ê': np.str_('e'), 'ë': np.str_('e'), 'î': np.str_('i'), 'ò': np.str_('o'), 'ô': np.str_('o'), 'ö': np.str_('o'), 'ù': np.str_('u'), 'û': np.str_('u'), 'ü': np.str_('u'), 'ē': np.str_('e'), 'ō': np.str_('o'), 'ū': np.str_('u'), 'ɉ': np.str_('j'), '̃': np.str_('�'), '̄': np.str_('�'), 'ͣ': np.str_('a'), 'ͤ': np.str_('e'), 'ͥ': np.str_('i'), 'ͦ': np.str_('o'), 'ͧ': np.str_('u'), 'ꝟ': np.str_('�'), 'ꝫ': np.str_('�'), 'ꝭ': np.str_('�')}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 2361 items.\n",
      "Training Dataset : Lines 2361, Characters 338432, CER 0.0293%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 30\n",
      "\n",
      "Train Accuracy: 0.9945631618759455 \n",
      "Valid Accuracy: 0.9940025327949153 \n",
      "Train Loss: 0.017764483216464916 \n",
      "Valid Loss: 0.02236409637071535 \n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': np.str_(' '), '!': np.str_('!'), '(': np.str_('('), ')': np.str_(')'), ',': np.str_(','), '-': np.str_('-'), '.': np.str_('.'), '/': np.str_('/'), '0': np.str_('0'), '1': np.str_('1'), '2': np.str_('2'), '3': np.str_('3'), '4': np.str_('4'), '5': np.str_('5'), '6': np.str_('6'), '7': np.str_('7'), '8': np.str_('8'), '9': np.str_('9'), ':': np.str_(':'), '?': np.str_('?'), 'A': np.str_('A'), 'B': np.str_('B'), 'C': np.str_('C'), 'D': np.str_('D'), 'E': np.str_('E'), 'F': np.str_('F'), 'G': np.str_('G'), 'H': np.str_('H'), 'I': np.str_('I'), 'J': np.str_('J'), 'K': np.str_('K'), 'L': np.str_('L'), 'M': np.str_('M'), 'N': np.str_('N'), 'O': np.str_('O'), 'P': np.str_('P'), 'R': np.str_('R'), 'S': np.str_('S'), 'T': np.str_('T'), 'U': np.str_('U'), 'V': np.str_('V'), 'W': np.str_('W'), 'X': np.str_('X'), 'Y': np.str_('Y'), 'Z': np.str_('Z'), '\\\\': np.str_('\\\\'), 'a': np.str_('a'), 'b': np.str_('b'), 'c': np.str_('c'), 'd': np.str_('d'), 'e': np.str_('e'), 'f': np.str_('f'), 'g': np.str_('g'), 'h': np.str_('h'), 'i': np.str_('i'), 'j': np.str_('j'), 'k': np.str_('k'), 'l': np.str_('l'), 'm': np.str_('m'), 'n': np.str_('n'), 'o': np.str_('o'), 'p': np.str_('p'), 'q': np.str_('q'), 'r': np.str_('r'), 's': np.str_('s'), 't': np.str_('t'), 'u': np.str_('u'), 'v': np.str_('v'), 'w': np.str_('w'), 'x': np.str_('x'), 'y': np.str_('y'), 'z': np.str_('z'), '¬': np.str_('!'), 'À': np.str_('A'), 'Ä': np.str_('A'), 'Ò': np.str_('O'), 'Ù': np.str_('U'), 'Û': np.str_('U'), 'à': np.str_('a'), 'â': np.str_('a'), 'ä': np.str_('a'), 'è': np.str_('e'), 'ê': np.str_('e'), 'ë': np.str_('e'), 'î': np.str_('i'), 'ò': np.str_('o'), 'ô': np.str_('o'), 'ö': np.str_('o'), 'ù': np.str_('u'), 'û': np.str_('u'), 'ü': np.str_('u'), 'ē': np.str_('e'), 'ō': np.str_('o'), 'ū': np.str_('u'), 'ɉ': np.str_('j'), '̃': np.str_('�'), '̄': np.str_('�'), 'ͣ': np.str_('a'), 'ͤ': np.str_('e'), 'ͥ': np.str_('i'), 'ͦ': np.str_('o'), 'ͧ': np.str_('u'), 'ꝟ': np.str_('�'), 'ꝫ': np.str_('�'), 'ꝭ': np.str_('�')}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 2361 items.\n",
      "Training Dataset : Lines 2361, Characters 338432, CER 0.0293%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 24\n",
      "\n",
      "Train Accuracy: 0.9943829188729199 \n",
      "Valid Accuracy: 0.9936082769826289 \n",
      "Train Loss: 0.01950453555820793 \n",
      "Valid Loss: 0.02252568135197277 \n",
      "\n",
      "Running on cuda\n",
      "Corpus loaded: 3211 lines, 428196 characters, 107 unique characters.\n",
      "This is a test. with some � characters. and their � replacements.\n",
      "LemmatizerBMP(mapping_dict={' ': np.str_(' '), '!': np.str_('!'), '(': np.str_('('), ')': np.str_(')'), ',': np.str_(','), '-': np.str_('-'), '.': np.str_('.'), '/': np.str_('/'), '0': np.str_('0'), '1': np.str_('1'), '2': np.str_('2'), '3': np.str_('3'), '4': np.str_('4'), '5': np.str_('5'), '6': np.str_('6'), '7': np.str_('7'), '8': np.str_('8'), '9': np.str_('9'), ':': np.str_(':'), '?': np.str_('?'), 'A': np.str_('A'), 'B': np.str_('B'), 'C': np.str_('C'), 'D': np.str_('D'), 'E': np.str_('E'), 'F': np.str_('F'), 'G': np.str_('G'), 'H': np.str_('H'), 'I': np.str_('I'), 'J': np.str_('J'), 'K': np.str_('K'), 'L': np.str_('L'), 'M': np.str_('M'), 'N': np.str_('N'), 'O': np.str_('O'), 'P': np.str_('P'), 'R': np.str_('R'), 'S': np.str_('S'), 'T': np.str_('T'), 'U': np.str_('U'), 'V': np.str_('V'), 'W': np.str_('W'), 'X': np.str_('X'), 'Y': np.str_('Y'), 'Z': np.str_('Z'), '\\\\': np.str_('\\\\'), 'a': np.str_('a'), 'b': np.str_('b'), 'c': np.str_('c'), 'd': np.str_('d'), 'e': np.str_('e'), 'f': np.str_('f'), 'g': np.str_('g'), 'h': np.str_('h'), 'i': np.str_('i'), 'j': np.str_('j'), 'k': np.str_('k'), 'l': np.str_('l'), 'm': np.str_('m'), 'n': np.str_('n'), 'o': np.str_('o'), 'p': np.str_('p'), 'q': np.str_('q'), 'r': np.str_('r'), 's': np.str_('s'), 't': np.str_('t'), 'u': np.str_('u'), 'v': np.str_('v'), 'w': np.str_('w'), 'x': np.str_('x'), 'y': np.str_('y'), 'z': np.str_('z'), '¬': np.str_('!'), 'À': np.str_('A'), 'Ä': np.str_('A'), 'Ò': np.str_('O'), 'Ù': np.str_('U'), 'Û': np.str_('U'), 'à': np.str_('a'), 'â': np.str_('a'), 'ä': np.str_('a'), 'è': np.str_('e'), 'ê': np.str_('e'), 'ë': np.str_('e'), 'î': np.str_('i'), 'ò': np.str_('o'), 'ô': np.str_('o'), 'ö': np.str_('o'), 'ù': np.str_('u'), 'û': np.str_('u'), 'ü': np.str_('u'), 'ē': np.str_('e'), 'ō': np.str_('o'), 'ū': np.str_('u'), 'ɉ': np.str_('j'), '̃': np.str_('�'), '̄': np.str_('�'), 'ͣ': np.str_('a'), 'ͤ': np.str_('e'), 'ͥ': np.str_('i'), 'ͦ': np.str_('o'), 'ͧ': np.str_('u'), 'ꝟ': np.str_('�'), 'ꝫ': np.str_('�'), 'ꝭ': np.str_('�')}, unknown_chr='�') Num outputs: 73 Num inputs: 105 assignments: 105\n",
      "Dataset loaded: Items 2952, CER 0.0294% , Input alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz, Output alphabet:  !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\n",
      "Reduced training dataset size from 2361 to 2361 items.\n",
      "Training Dataset : Lines 2361, Characters 338432, CER 0.0293%\n",
      "Validation Dataset : Lines 591, Characters 83702, CER 0.0299%\n",
      "Indicative Validation Sample:\n",
      "Sample 0:\n",
      "Source        : Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Source decoded: Hanns Muellern von Waltzhuet, dem schaffner da selbs, an stat und innamen der erwirdigen, gaistlichen froewen,\n",
      "Target        : Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Target decoded: Hanns Muͤllern von Waltzhuͤt, dem schaffner da selbs, an stat und innämen der erwirdigen, gaistlichen froͤwen,\n",
      "Source Tensor: tensor([28, 47, 60, 60, 65,  1, 33, 67, 51, 58, 58, 51, 64, 60,  1, 68, 61, 60,\n",
      "         1, 42, 47, 58, 66, 72, 54, 67, 51, 66,  5,  1, 50, 51, 59,  1, 65, 49,\n",
      "        54, 47, 52, 52, 60, 51, 64,  1, 50, 47,  1, 65, 51, 58, 48, 65,  5,  1,\n",
      "        47, 60,  1, 65, 66, 47, 66,  1, 67, 60, 50,  1, 55, 60, 60, 47, 59, 51,\n",
      "        60,  1, 50, 51, 64,  1, 51, 64, 69, 55, 64, 50, 55, 53, 51, 60,  5,  1,\n",
      "        53, 47, 55, 65, 66, 58, 55, 49, 54, 51, 60,  1, 52, 64, 61, 51, 69, 51,\n",
      "        60,  5])\n",
      "Target Tensor: tensor([ 28,  47,  60,  60,  65,   1,  33,  67, 101,  58,  58,  51,  64,  60,\n",
      "          1,  68,  61,  60,   1,  42,  47,  58,  66,  72,  54,  67, 101,  66,\n",
      "          5,   1,  50,  51,  59,   1,  65,  49,  54,  47,  52,  52,  60,  51,\n",
      "         64,   1,  50,  47,   1,  65,  51,  58,  48,  65,   5,   1,  47,  60,\n",
      "          1,  65,  66,  47,  66,   1,  67,  60,  50,   1,  55,  60,  60,  81,\n",
      "         59,  51,  60,   1,  50,  51,  64,   1,  51,  64,  69,  55,  64,  50,\n",
      "         55,  53,  51,  60,   5,   1,  53,  47,  55,  65,  66,  58,  55,  49,\n",
      "         54,  51,  60,   1,  52,  64,  61, 101,  69,  51,  60,   5])\n",
      "\n",
      "\n",
      "Validation set mapper CER 0.0294\n",
      "DemapperLSTM(input_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz', output_alphabet=' !(),-./0123456789:?ABCDEFGHIJKLMNOPRSTUVWXYZ\\\\abcdefghijklmnopqrstuvwxyz¬ÀÄÒÙÛàâäèêëîòôöùûüēōūɉˀ̀̃̄ͣͤͥͦͧ₎✳ꝟꝫꝭ\\uf2f7', hidden_sizes=[128, 128, 128], dropout=[0.1, 0.1, 0.1])Epoch: 16\n",
      "\n",
      "Train Accuracy: 0.9939426531770046 \n",
      "Valid Accuracy: 0.9935724355451483 \n",
      "Train Loss: 0.02204703051368191 \n",
      "Valid Loss: 0.022508319497444113 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for max_trainset_sz in [20, 40, 60, 80, 100, 125, 150, 175, 200, 250, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000, 2500, 3000,4000,5000,7500]:\n",
    "#for max_trainset_sz in [4000,5000,7500, 10000,15000]:\n",
    "    corpus_name = f\"koening{max_trainset_sz:04d}\"\n",
    "    corpus_files = set(glob.glob(\"../../tmp/koeningsfelden/koenigsfelden_1308-1662_expanded/U-17_07*txt\"))\n",
    "    nb_epochs=int(math.ceil(60000/max_trainset_sz))*2\n",
    "    \n",
    "\n",
    "    archname = \"simple\"\n",
    "    hidden_sizes=\"128,128,128\" ; dropouts=\"0.1,0.1,0.1\"\n",
    "\n",
    "    mapname = \"mufi2ascii\"\n",
    "    input_alphabet=pylelemmatize.charset.mufibmp\n",
    "    output_alphabet=''.join(set(pylelemmatize.charset.ascii))\n",
    "    custom_map=\"\"\n",
    "    output_model_path=f\"./models/curve_{mapname}_{corpus_name}_{archname}.pt\"         \n",
    "        \n",
    "    if train_models:\n",
    "        pylelemmatize.main_train_one2one(argv=[\"NA\"],\n",
    "            input_alphabet=input_alphabet, output_alphabet=output_alphabet, custom_map=custom_map,\n",
    "            corpus_files=corpus_files, max_trainset_sz=max_trainset_sz,\n",
    "            nb_epochs=nb_epochs,\n",
    "            hidden_sizes=hidden_sizes, dropouts=dropouts,\n",
    "            pseudo_batch_size=10, lr=0.003,\n",
    "            train_test_split=0.8,\n",
    "            output_model_path=output_model_path\n",
    "        )\n",
    "    print(f\"COMPLETED: {output_model_path}\\n\\n\\t\\t\\t#########################\\n\\n\\n\\n\\n\\\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7ef03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (p311)",
   "language": "python",
   "name": "p311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
